<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Numerical Optimisation of Univariate Functions | Optimisation II APPM2007</title>
  <meta name="description" content="Course notes for Optimisation II at the University of the Witwatersrand" />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Numerical Optimisation of Univariate Functions | Optimisation II APPM2007" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="wits_high_def-min.png" />
  <meta property="og:description" content="Course notes for Optimisation II at the University of the Witwatersrand" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Numerical Optimisation of Univariate Functions | Optimisation II APPM2007" />
  
  <meta name="twitter:description" content="Course notes for Optimisation II at the University of the Witwatersrand" />
  <meta name="twitter:image" content="wits_high_def-min.png" />

<meta name="author" content="Dr Matthew Woolway" />


<meta name="date" content="2019-06-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="numerical-solutions-to-nonlinear-equations.html">
<link rel="next" href="multivariate-unconstrained-optimisation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimisation II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course Outline</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-structure-and-details"><i class="fa fa-check"></i>Course Structure and Details</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-assessment"><i class="fa fa-check"></i>Course Assessment</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-topics"><i class="fa fa-check"></i>Course Topics</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#hardware-requirements"><i class="fa fa-check"></i>Hardware Requirements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html"><i class="fa fa-check"></i><b>1</b> Definition and General Concepts</a><ul>
<li class="chapter" data-level="1.1" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#nonlinear-optimisation"><i class="fa fa-check"></i><b>1.1</b> Nonlinear Optimisation</a></li>
<li class="chapter" data-level="1.2" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#general-statement-of-a-optimisation-problem"><i class="fa fa-check"></i><b>1.2</b> General Statement of a Optimisation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#important-optimisation-concepts"><i class="fa fa-check"></i><b>1.3</b> Important Optimisation Concepts</a><ul>
<li class="chapter" data-level="1.3.1" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#definitions"><i class="fa fa-check"></i><b>1.3.1</b> Definitions</a></li>
<li class="chapter" data-level="1.3.2" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#convexity"><i class="fa fa-check"></i><b>1.3.2</b> Convexity</a></li>
<li class="chapter" data-level="1.3.3" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#exercises"><i class="fa fa-check"></i><b>1.3.3</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html"><i class="fa fa-check"></i><b>2</b> One Dimensional Unconstrained and Bound Constrained Problems</a><ul>
<li class="chapter" data-level="2.1" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#unimodal-and-multimodal"><i class="fa fa-check"></i><b>2.1</b> Unimodal and Multimodal</a></li>
<li class="chapter" data-level="2.2" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#convex-functions"><i class="fa fa-check"></i><b>2.2</b> Convex Functions</a></li>
<li class="chapter" data-level="2.3" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#global-extrema"><i class="fa fa-check"></i><b>2.3</b> Global Extrema</a></li>
<li class="chapter" data-level="2.4" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#necessary-and-sufficient-conditions"><i class="fa fa-check"></i><b>2.4</b> Necessary and Sufficient Conditions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#exercises-1"><i class="fa fa-check"></i><b>2.4.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html"><i class="fa fa-check"></i><b>3</b> Numerical Solutions to Nonlinear Equations</a><ul>
<li class="chapter" data-level="3.1" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#newtons-method"><i class="fa fa-check"></i><b>3.1</b> Newton’s Method</a><ul>
<li class="chapter" data-level="3.1.1" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#advantages-and-disadvantages-of-newtons-method"><i class="fa fa-check"></i><b>3.1.1</b> Advantages and Disadvantages of Newton’s Method</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#secant-method"><i class="fa fa-check"></i><b>3.2</b> Secant Method</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#exercises-2"><i class="fa fa-check"></i><b>3.2.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html"><i class="fa fa-check"></i><b>4</b> Numerical Optimisation of Univariate Functions</a><ul>
<li class="chapter" data-level="4.1" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#techniques-using-function-evaluations"><i class="fa fa-check"></i><b>4.1</b> Techniques Using Function Evaluations</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#bisection-method"><i class="fa fa-check"></i><b>4.1.1</b> Bisection Method</a></li>
<li class="chapter" data-level="4.1.2" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#golden-search-method"><i class="fa fa-check"></i><b>4.1.2</b> Golden Search Method</a></li>
<li class="chapter" data-level="4.1.3" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#exercises-3"><i class="fa fa-check"></i><b>4.1.3</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html"><i class="fa fa-check"></i><b>5</b> Multivariate Unconstrained Optimisation</a><ul>
<li class="chapter" data-level="5.1" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#terminology-for-functions-of-several-variables"><i class="fa fa-check"></i><b>5.1</b> Terminology for Functions of Several Variables</a></li>
<li class="chapter" data-level="5.2" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#a-line-in-a-particular-direction-in-the-context-of-optimisation"><i class="fa fa-check"></i><b>5.2</b> A Line in a Particular Direction in the Context of Optimisation</a></li>
<li class="chapter" data-level="5.3" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#taylor-series-for-multivariate-function"><i class="fa fa-check"></i><b>5.3</b> Taylor Series for Multivariate Function</a></li>
<li class="chapter" data-level="5.4" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#quadratic-forms"><i class="fa fa-check"></i><b>5.4</b> Quadratic Forms</a></li>
<li class="chapter" data-level="5.5" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#stationary-points"><i class="fa fa-check"></i><b>5.5</b> Stationary Points</a><ul>
<li class="chapter" data-level="5.5.1" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#tests-for-positive-definiteness"><i class="fa fa-check"></i><b>5.5.1</b> Tests for Positive Definiteness</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#necessary-and-sufficient-conditions-1"><i class="fa fa-check"></i><b>5.6</b> Necessary and Sufficient Conditions</a><ul>
<li class="chapter" data-level="5.6.1" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#exercises-4"><i class="fa fa-check"></i><b>5.6.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html"><i class="fa fa-check"></i><b>6</b> Gradient Methods for Unconstrained Optimisation</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#general-line-search-techniques-used-in-unconstrained-multivariate-minimisation"><i class="fa fa-check"></i><b>6.1</b> General Line Search Techniques used in Unconstrained Multivariate Minimisation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#challenges-in-computing-step-length-alphak"><i class="fa fa-check"></i><b>6.1.1</b> Challenges in Computing Step Length <span class="math inline">\(\alpha^k\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#exact-and-inexact-line-search"><i class="fa fa-check"></i><b>6.2</b> Exact and Inexact Line Search</a><ul>
<li class="chapter" data-level="6.2.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#algorithmic-structure"><i class="fa fa-check"></i><b>6.2.1</b> Algorithmic Structure</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-descent-condition"><i class="fa fa-check"></i><b>6.3</b> The Descent Condition</a></li>
<li class="chapter" data-level="6.4" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-direction-of-greatest-reduction"><i class="fa fa-check"></i><b>6.4</b> The Direction of Greatest Reduction</a></li>
<li class="chapter" data-level="6.5" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-method-of-steepest-descent"><i class="fa fa-check"></i><b>6.5</b> The Method of Steepest Descent</a><ul>
<li class="chapter" data-level="6.5.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#steepest-descent-algorithm"><i class="fa fa-check"></i><b>6.5.1</b> Steepest Descent Algorithm</a></li>
<li class="chapter" data-level="6.5.2" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#convergence-criteria"><i class="fa fa-check"></i><b>6.5.2</b> Convergence Criteria</a></li>
<li class="chapter" data-level="6.5.3" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#inexact-line-search"><i class="fa fa-check"></i><b>6.5.3</b> Inexact Line Search</a></li>
<li class="chapter" data-level="6.5.4" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#exercises-5"><i class="fa fa-check"></i><b>6.5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-gradient-descent-algorithm-and-machine-learning"><i class="fa fa-check"></i><b>6.6</b> The Gradient Descent Algorithm and Machine Learning</a><ul>
<li class="chapter" data-level="6.6.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#basic-example"><i class="fa fa-check"></i><b>6.6.1</b> Basic Example</a></li>
<li class="chapter" data-level="6.6.2" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#adaptive-step-size"><i class="fa fa-check"></i><b>6.6.2</b> Adaptive Step-Size</a></li>
<li class="chapter" data-level="6.6.3" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#decreasing-step-size"><i class="fa fa-check"></i><b>6.6.3</b> Decreasing Step-Size</a></li>
<li class="chapter" data-level="6.6.4" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>6.6.4</b> Stochastic Gradient Descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html"><i class="fa fa-check"></i><b>7</b> Newton and Quasi-Newton Methods</a><ul>
<li class="chapter" data-level="7.1" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#the-modified-newton-method"><i class="fa fa-check"></i><b>7.1</b> The Modified Newton Method</a></li>
<li class="chapter" data-level="7.2" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#convergence-of-newtons-method-for-quadratic-functions"><i class="fa fa-check"></i><b>7.2</b> Convergence of Newton’s Method for Quadratic Functions</a></li>
<li class="chapter" data-level="7.3" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#quasi-newton-methods"><i class="fa fa-check"></i><b>7.3</b> Quasi-Newton Methods</a><ul>
<li class="chapter" data-level="7.3.1" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#the-dfp-quasi-newton-method"><i class="fa fa-check"></i><b>7.3.1</b> The DFP Quasi-Newton Method</a></li>
<li class="chapter" data-level="7.3.2" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#exercises-6"><i class="fa fa-check"></i><b>7.3.2</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html"><i class="fa fa-check"></i><b>8</b> Direct Search Methods for Unconstrained Optimisation</a><ul>
<li class="chapter" data-level="8.1" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#random-walk-method"><i class="fa fa-check"></i><b>8.1</b> Random Walk Method</a></li>
<li class="chapter" data-level="8.2" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#downhill-simplex-method-of-nelder-and-mead"><i class="fa fa-check"></i><b>8.2</b> Downhill Simplex Method of Nelder and Mead</a></li>
<li class="chapter" data-level="8.3" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#rosenbrock-function-example"><i class="fa fa-check"></i><b>8.3</b> Rosenbrock Function Example</a><ul>
<li class="chapter" data-level="8.3.1" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#exercises-7"><i class="fa fa-check"></i><b>8.3.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lagrangian-multipliers-for-constraint-optimisation.html"><a href="lagrangian-multipliers-for-constraint-optimisation.html"><i class="fa fa-check"></i><b>9</b> Lagrangian Multipliers for Constraint Optimisation</a><ul>
<li class="chapter" data-level="9.0.1" data-path="lagrangian-multipliers-for-constraint-optimisation.html"><a href="lagrangian-multipliers-for-constraint-optimisation.html#example-12"><i class="fa fa-check"></i><b>9.0.1</b> Example</a></li>
<li class="chapter" data-level="9.0.2" data-path="lagrangian-multipliers-for-constraint-optimisation.html"><a href="lagrangian-multipliers-for-constraint-optimisation.html#exercises-8"><i class="fa fa-check"></i><b>9.0.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">University of the Witwatersrand</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimisation II APPM2007</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="numerical-optimisation-of-univariate-functions" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Numerical Optimisation of Univariate Functions</h1>
<p>The simplest functions with which to begin a study of non-linear optimisation methods are those with a single independent variable. Although the minimisation of univariate functions is in itself of some practical importance, the main area of application for these techniques is as a subproblem of multivariate minimisation.</p>
<p>There are functions to be minimised where the variable <span class="math inline">\(x\)</span> is unrestricted (say, <span class="math inline">\(x\in \mathbb{R}\)</span>); there are also functions to be optimised over a finite interval (in <span class="math inline">\(n\)</span>-dimension it is a box). Single variable optimization in a finite interval is important because of its application is in multi-variable optimisation. In this chapter we will consider one dimensional optimisation.</p>
<p>If one needs to find the maximum or minimum (i.e. the optimal) value of a function <span class="math inline">\(f(x)\)</span> on the interval <span class="math inline">\([a,b]\)</span> the procedure would be:</p>
<ol style="list-style-type: decimal">
<li>Find all turning (stationary) points of <span class="math inline">\(f(x)\)</span> (assuming <span class="math inline">\(f(x)\)</span> is differentiable) on <span class="math inline">\([a, b]\)</span> and then decide the optimum.</li>
<li>Find the optimal turning point of <span class="math inline">\(f(x)\)</span> on <span class="math inline">\([a,b]\)</span>.</li>
</ol>
<p>Generally it may be difficult/impossible/tiresome to implement (i) analytically, so we resort to the computer and an appropriate numerical method to find an optimal (hopefully the best estimate!) solution of an univariate function. In the next section we introduce some numerical techniques. The numerical approach is mandatory when the function <span class="math inline">\(f(x)\)</span> is not given explicitly.</p>
<p>In many cases when one would like to find the minimiser of a function <span class="math inline">\(f(x)\)</span> but neither <span class="math inline">\(f(x)\)</span> nor <span class="math inline">\(f&#39;(x)\)</span> are given (or known) explicitly, then the numerical approaches viz : polynomial interpolations or function comparison methods are used. These are the univariate optimisation used as <strong>line search</strong> in multivariate optimisation.</p>
<div id="techniques-using-function-evaluations" class="section level2">
<h2><span class="header-section-number">4.1</span> Techniques Using Function Evaluations</h2>
<div id="bisection-method" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Bisection Method</h3>
<p>We assume that an interval <span class="math inline">\([a,b]\)</span> is given and that a local minimum <span class="math inline">\(x^*\in [a,b].\)</span> When the first derivative of the objective function <span class="math inline">\(f(x)\)</span> is known at <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, it is necessary to evaluate function information at only one interior point in order to reduce this interval. This is because it is possible to decide if any interval brackets a minimum simply by looking at the function values <span class="math inline">\(f(a), f(b)\)</span> and <span class="math inline">\(f&#39;(a), f&#39;(b)\)</span> at extreme points <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The conditions that to be satisfied are:</p>
<ul>
<li><span class="math inline">\(f&#39;(a)&lt;0\)</span> and <span class="math inline">\(f&#39;(b)&gt;0\)</span>.</li>
<li><span class="math inline">\(f&#39;(a)&lt;0\)</span> and <span class="math inline">\(f(b)&gt;f(a)\)</span>.</li>
<li><span class="math inline">\(f&#39;(a)&gt;0\)</span> and <span class="math inline">\(f&#39;(b)&gt;0\)</span> and <span class="math inline">\(f(b)&lt;f(a)\)</span>.</li>
</ul>
<p>These three situations are illustrated in the Figure below. The next step of the bisection method is to reduce the interval. At the <span class="math inline">\(k\)</span>-th iteration we have an interval <span class="math inline">\([a_k,b_k]\)</span> and the mid-point <span class="math inline">\(c_k=\frac{1}{2}(a_k+b_k)\)</span> is computed. The next interval will be called <span class="math inline">\([a_{k+1},b_{k+1}]\)</span> which is either <span class="math inline">\([a_k,c_k]\)</span> or <span class="math inline">\([c_k,b_k]\)</span> depending on which interval brackets the minimum. The process continues until two consecutive interval produces minima
which are within an acceptable tolerance.</p>
<p><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-26-1.png" /><!-- --></p>
<p><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-27-1.png" /><!-- --></p>
<p><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-28-1.png" /><!-- --></p>
<hr />
<div id="exercise" class="section level4">
<h4><span class="header-section-number">4.1.1.1</span> Exercise</h4>
<p>Find the minimum value of:
<span class="math display">\[
f(x) = -\frac{1}{3}x^3 - \frac{1}{2} x^2 + 2x - 5,
\]</span>
over the domain <span class="math inline">\([-3, \ 0]\)</span> using the bisection method. The problem has a minimum of value of -8.33 at <span class="math inline">\(x = 2\)</span>.</p>
<hr />
</div>
</div>
<div id="golden-search-method" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Golden Search Method</h3>
<p>Suppose <span class="math inline">\(f: \mathbb{R} \to \mathbb{R}\)</span> on the interval <span class="math inline">\([a_0,b_0]\)</span> and <span class="math inline">\(f\)</span> has only one minimum (we say <span class="math inline">\(f\)</span> is <em>unimodal</em> at <span class="math inline">\(x^*.\)</span> The problem is to locate <span class="math inline">\(x^*\)</span>. The method we now discuss is based on evaluating the objective function at different points in the interval <span class="math inline">\([a_0,b_0]\)</span>. We choose these points in such a way that an approximation to the minimiser of <span class="math inline">\(f\)</span> may be achieved in as few evaluation as possible. Our goal is to progressively narrow down the range of the subinterval containing <span class="math inline">\(x^*\)</span>. If we evaluate <span class="math inline">\(f\)</span> at only one intermediate point of the interval <span class="math inline">\([a_0,b_0]\)</span>, we cannot narrow the range within which we know the minimiser is located. We have to evaluate <span class="math inline">\(f\)</span> at two intermediate points in such a way that the reduction in the range is symmetrical, in the sense that <span class="math inline">\(a_1 - a_0 = b_0 - b_1 = \rho(b_0 - a_0),\)</span> where <span class="math inline">\(\rho &lt; {1\over 2}\)</span> to keep <span class="math inline">\(a_1\)</span> ‘near’ to <span class="math inline">\(b_0.\)</span> We then evaluate <span class="math inline">\(f\)</span> at the intermediate points. If <span class="math inline">\(f(a_1)&lt;f(b_1)\)</span>, then the minimiser must lie in the range <span class="math inline">\([a_0,b_1]\)</span>. If, on the other hand, <span class="math inline">\(f(a_1)\ge f(b_1)\)</span>, then the minimiser located in the range <span class="math inline">\([a_1,b_0]\)</span>. Starting with the reduced range of uncertainty we can repeat the process and similarly find two new points <span class="math inline">\(a_2\)</span> and <span class="math inline">\(b_2\)</span>, using the same value of <span class="math inline">\(\rho\)</span> as before. However, we would like to minimise the number of function evaluations while reducing the width of the interval of uncertainty. Suppose that <span class="math inline">\(f(a_1)&lt;f(b_1)\)</span>. Then we know that <span class="math inline">\(x^*\in [a_0,b_1]\)</span>. Because <span class="math inline">\(a_1\)</span> is already in the uncertainty interval and <span class="math inline">\(f(a_1)\)</span> is known, we can use these information. We can make <span class="math inline">\(a_1\)</span> coincide with <span class="math inline">\(b_2\)</span>. Thus, only one new evaluation of <span class="math inline">\(f\)</span> at <span class="math inline">\(a_2\)</span> would be necessary. We can now calculate the value of <span class="math inline">\(\rho\)</span> that results in only one new evaluation of <span class="math inline">\(f\)</span>. To save algebra we will assume that <span class="math inline">\(b_0 - a_0 = 1.\)</span> Then, to have only one new evaluation of <span class="math inline">\(f\)</span> it is enough to choose <span class="math inline">\(\rho\)</span> so that:</p>
<p><span class="math display">\[
\rho(b_1-a_0)=b_1-b_2.
\]</span>
Because <span class="math inline">\(b_1-a_0=1-\rho\)</span> and <span class="math inline">\(b_1-b_2=1-2\rho\)</span>, we have:
<span class="math display" id="eq:golden">\[\begin{equation}
\rho(1-\rho)=1-2\rho.\tag{4.1}
\end{equation}\]</span>
The solutions of Equation <a href="numerical-optimisation-of-univariate-functions.html#eq:golden">(4.1)</a> are <span class="math inline">\(\frac{3\pm \sqrt{5}}{2}\)</span>, because <span class="math inline">\(\rho&lt;0.5\)</span> we take
<span class="math inline">\(\rho=\frac{3-\sqrt{5}}{2}\)</span>.</p>
<p>Therefore,
<span class="math display">\[\begin{eqnarray}
a_1&amp;=&amp;a_0+\rho(b_0-a_0)\\
b_1&amp;=&amp;b_0-\rho(b_0-a_0)
\end{eqnarray}\]</span>
Somewhere in the intervals <span class="math inline">\([a_0,a_1), [a_1,b_1), [b_1,b_0],\)</span> lies the point <span class="math inline">\(x^*.\)</span></p>
<ul>
<li>If <span class="math inline">\(f(a_1) &lt; f(b_1), \ \ x^*\in[a_0,b_1].\)</span></li>
<li>If <span class="math inline">\(f(a_1) \ge f(b_1), \ \ x^*\in[a_1,b_0].\)</span></li>
</ul>
<p>This forms the basis of a search algorithm since the technique is applied again on the reduced interval.</p>
<hr />
<div id="example-1" class="section level4">
<h4><span class="header-section-number">4.1.2.1</span> Example</h4>
<p>Use the four iterations Golden Section search to find the value of <span class="math inline">\(x\)</span> that minimizes:
<span class="math display">\[f(x)=x^4-14x^3+60x^2-70x,\]</span>
on the domain <span class="math inline">\([0, \ 2]\)</span>.</p>
<p><strong>Answer:</strong></p>
<p><strong>Iteration 1:</strong></p>
<p>We evaluate <span class="math inline">\(f\)</span> in two intermediate points <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span>. We have:</p>
<p><span class="math display">\[\begin{array}{c}
a_1=a_0+\rho(b_0-a_0)=0.763,\\
b_1=a_0+(1-\rho)(b_0-a_0)=1.236.
\end{array}\]</span></p>
<p>We compute
<span class="math display">\[\begin{array}{c}
f(a_1)=-24.36,\\
f(b_1)=-18.96.
\end{array}\]</span></p>
<p>Thus we have <span class="math inline">\(f(a_1)&lt;f(b_1)\)</span>, and so the uncertainty interval is reduced
to <span class="math inline">\([a_0,b_1]=[0,1.236]\)</span>.</p>
<p><strong>Iteration 2:</strong></p>
<p>We choose <span class="math inline">\(b_2\)</span> to coincide with <span class="math inline">\(a_1\)</span>, and <span class="math inline">\(f\)</span> need only
to be evaluated at one new point
<span class="math display">\[a_2=a_0+\rho(b_1-a_0)=0.4721.\]</span> Now we have:</p>
<p><span class="math display">\[\begin{array}{c}
f(a_2)=-21.10,\\
f(b_2)=-24.36.
\end{array}\]</span></p>
<p>Now, <span class="math inline">\(f(b_2)&lt;f(a_2)\)</span>, and so the uncertainty interval is reduced
to <span class="math inline">\([a_2,b_1]=[0.4721,1.236]\)</span>.</p>
<p><strong>Iteration 3:</strong></p>
<p>We set <span class="math inline">\(a_3=b_2\)</span>, and compute <span class="math inline">\(b_3\)</span>:</p>
<p><span class="math display">\[b_3=a_2+(1-\rho)(b_1-a_2)=0.9443.\]</span></p>
<p>We have:</p>
<p><span class="math display">\[\begin{array}{c}
f(a_3)=-24.36,\\
f(b_3)=-23.59.
\end{array}\]</span></p>
<p>So we have <span class="math inline">\(f(b_3)&gt;f(a_3)\)</span>. Hence, the new interval is <span class="math inline">\([a_2,b_3]=[0.472,0.944]\)</span>.</p>
<p><strong>Iteration 4:</strong></p>
<p>We set <span class="math inline">\(b_4=a_3\)</span>, and compute <span class="math inline">\(a_4\)</span>:</p>
<p><span class="math display">\[a_4=a_2+\rho(b_3-a_2)=0.6525.\]</span></p>
<p>We have:</p>
<p><span class="math display">\[\begin{array}{c}
f(a_4)=-23.84,\\
f(b_4)=-24.36.
\end{array}\]</span></p>
<p>Since <span class="math inline">\(f(a_4)&gt;f(b_4)\)</span>. Thus the value of <span class="math inline">\(x\)</span> that minimizes <span class="math inline">\(f\)</span> is
located in the interval <span class="math display">\[[a_4,b_3]=[0.652,0.944].\]</span></p>
<pre class="sourceCode python"><code class="sourceCode python">plt.show()</code></pre>
<p><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-30-1.png" /><!-- --></p>
<p><strong>Goldern Search Pseudocode</strong></p>
<pre class="sourceCode octave"><code class="sourceCode octave">Inputs [a, b], tol and rho = (<span class="fl">1</span> + <span class="fu">sqrt</span>(<span class="fl">5</span>))/<span class="fl">2</span>
Let c = b + (a - b)/rho and d = a + (b - a)/rho
while <span class="fu">abs</span>(c - d) &gt; tol
    if f(c) &lt; f(d) do
        (b, f(b)) &lt;- (d, f(d)) and (d, f(d)) &lt;- (c, f(c))
        Update c = b + (a - b)/rho and f(c)
    else
        (a, f(a)) &lt;- (c, f(c)) and (c, f(c)) &lt;- (d, f(d))
    end
end
Return (b + a)/<span class="fl">2</span></code></pre>
<hr />
</div>
</div>
<div id="exercises-3" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Exercises</h3>
<ol style="list-style-type: decimal">
<li>Find the minimum value of the one dimensional function <span class="math inline">\(f(x) = x^2 - 3x\exp(-x)\)</span>, over [0, 1], using:
<ul>
<li>Bisection Method</li>
<li>Golden Search Method</li>
</ul></li>
</ol>
<hr />

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="numerical-solutions-to-nonlinear-equations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multivariate-unconstrained-optimisation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-numerical_optimisation_of_univariate_functions.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["OptimisationII_notes.pdf", "OptimisationII_notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
