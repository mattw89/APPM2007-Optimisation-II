<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Direct Search Methods for Unconstrained Optimisation | Optimisation II APPM2007</title>
  <meta name="description" content="Course notes for Optimisation II at the University of the Witwatersrand" />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Direct Search Methods for Unconstrained Optimisation | Optimisation II APPM2007" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="wits_high_def-min.png" />
  <meta property="og:description" content="Course notes for Optimisation II at the University of the Witwatersrand" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Direct Search Methods for Unconstrained Optimisation | Optimisation II APPM2007" />
  
  <meta name="twitter:description" content="Course notes for Optimisation II at the University of the Witwatersrand" />
  <meta name="twitter:image" content="wits_high_def-min.png" />

<meta name="author" content="Dr Matthew Woolway" />


<meta name="date" content="2019-06-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="newton-and-quasi-newton-methods.html">
<link rel="next" href="lagrangian-multipliers-for-constraint-optimisation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimisation II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course Outline</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-structure-and-details"><i class="fa fa-check"></i>Course Structure and Details</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-assessment"><i class="fa fa-check"></i>Course Assessment</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-topics"><i class="fa fa-check"></i>Course Topics</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#hardware-requirements"><i class="fa fa-check"></i>Hardware Requirements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html"><i class="fa fa-check"></i><b>1</b> Definition and General Concepts</a><ul>
<li class="chapter" data-level="1.1" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#nonlinear-optimisation"><i class="fa fa-check"></i><b>1.1</b> Nonlinear Optimisation</a></li>
<li class="chapter" data-level="1.2" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#general-statement-of-a-optimisation-problem"><i class="fa fa-check"></i><b>1.2</b> General Statement of a Optimisation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#important-optimisation-concepts"><i class="fa fa-check"></i><b>1.3</b> Important Optimisation Concepts</a><ul>
<li class="chapter" data-level="1.3.1" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#definitions"><i class="fa fa-check"></i><b>1.3.1</b> Definitions</a></li>
<li class="chapter" data-level="1.3.2" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#convexity"><i class="fa fa-check"></i><b>1.3.2</b> Convexity</a></li>
<li class="chapter" data-level="1.3.3" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#exercises"><i class="fa fa-check"></i><b>1.3.3</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html"><i class="fa fa-check"></i><b>2</b> One Dimensional Unconstrained and Bound Constrained Problems</a><ul>
<li class="chapter" data-level="2.1" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#unimodal-and-multimodal"><i class="fa fa-check"></i><b>2.1</b> Unimodal and Multimodal</a></li>
<li class="chapter" data-level="2.2" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#convex-functions"><i class="fa fa-check"></i><b>2.2</b> Convex Functions</a></li>
<li class="chapter" data-level="2.3" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#global-extrema"><i class="fa fa-check"></i><b>2.3</b> Global Extrema</a></li>
<li class="chapter" data-level="2.4" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#necessary-and-sufficient-conditions"><i class="fa fa-check"></i><b>2.4</b> Necessary and Sufficient Conditions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#exercises-1"><i class="fa fa-check"></i><b>2.4.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html"><i class="fa fa-check"></i><b>3</b> Numerical Solutions to Nonlinear Equations</a><ul>
<li class="chapter" data-level="3.1" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#newtons-method"><i class="fa fa-check"></i><b>3.1</b> Newton’s Method</a><ul>
<li class="chapter" data-level="3.1.1" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#advantages-and-disadvantages-of-newtons-method"><i class="fa fa-check"></i><b>3.1.1</b> Advantages and Disadvantages of Newton’s Method</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#secant-method"><i class="fa fa-check"></i><b>3.2</b> Secant Method</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#exercises-2"><i class="fa fa-check"></i><b>3.2.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html"><i class="fa fa-check"></i><b>4</b> Numerical Optimisation of Univariate Functions</a><ul>
<li class="chapter" data-level="4.1" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#techniques-using-function-evaluations"><i class="fa fa-check"></i><b>4.1</b> Techniques Using Function Evaluations</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#bisection-method"><i class="fa fa-check"></i><b>4.1.1</b> Bisection Method</a></li>
<li class="chapter" data-level="4.1.2" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#golden-search-method"><i class="fa fa-check"></i><b>4.1.2</b> Golden Search Method</a></li>
<li class="chapter" data-level="4.1.3" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#exercises-3"><i class="fa fa-check"></i><b>4.1.3</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html"><i class="fa fa-check"></i><b>5</b> Multivariate Unconstrained Optimisation</a><ul>
<li class="chapter" data-level="5.1" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#terminology-for-functions-of-several-variables"><i class="fa fa-check"></i><b>5.1</b> Terminology for Functions of Several Variables</a></li>
<li class="chapter" data-level="5.2" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#a-line-in-a-particular-direction-in-the-context-of-optimisation"><i class="fa fa-check"></i><b>5.2</b> A Line in a Particular Direction in the Context of Optimisation</a></li>
<li class="chapter" data-level="5.3" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#taylor-series-for-multivariate-function"><i class="fa fa-check"></i><b>5.3</b> Taylor Series for Multivariate Function</a></li>
<li class="chapter" data-level="5.4" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#quadratic-forms"><i class="fa fa-check"></i><b>5.4</b> Quadratic Forms</a></li>
<li class="chapter" data-level="5.5" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#stationary-points"><i class="fa fa-check"></i><b>5.5</b> Stationary Points</a><ul>
<li class="chapter" data-level="5.5.1" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#tests-for-positive-definiteness"><i class="fa fa-check"></i><b>5.5.1</b> Tests for Positive Definiteness</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#necessary-and-sufficient-conditions-1"><i class="fa fa-check"></i><b>5.6</b> Necessary and Sufficient Conditions</a><ul>
<li class="chapter" data-level="5.6.1" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#exercises-4"><i class="fa fa-check"></i><b>5.6.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html"><i class="fa fa-check"></i><b>6</b> Gradient Methods for Unconstrained Optimisation</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#general-line-search-techniques-used-in-unconstrained-multivariate-minimisation"><i class="fa fa-check"></i><b>6.1</b> General Line Search Techniques used in Unconstrained Multivariate Minimisation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#challenges-in-computing-step-length-alphak"><i class="fa fa-check"></i><b>6.1.1</b> Challenges in Computing Step Length <span class="math inline">\(\alpha^k\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#exact-and-inexact-line-search"><i class="fa fa-check"></i><b>6.2</b> Exact and Inexact Line Search</a><ul>
<li class="chapter" data-level="6.2.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#algorithmic-structure"><i class="fa fa-check"></i><b>6.2.1</b> Algorithmic Structure</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-descent-condition"><i class="fa fa-check"></i><b>6.3</b> The Descent Condition</a></li>
<li class="chapter" data-level="6.4" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-direction-of-greatest-reduction"><i class="fa fa-check"></i><b>6.4</b> The Direction of Greatest Reduction</a></li>
<li class="chapter" data-level="6.5" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-method-of-steepest-descent"><i class="fa fa-check"></i><b>6.5</b> The Method of Steepest Descent</a><ul>
<li class="chapter" data-level="6.5.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#steepest-descent-algorithm"><i class="fa fa-check"></i><b>6.5.1</b> Steepest Descent Algorithm</a></li>
<li class="chapter" data-level="6.5.2" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#convergence-criteria"><i class="fa fa-check"></i><b>6.5.2</b> Convergence Criteria</a></li>
<li class="chapter" data-level="6.5.3" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#inexact-line-search"><i class="fa fa-check"></i><b>6.5.3</b> Inexact Line Search</a></li>
<li class="chapter" data-level="6.5.4" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#exercises-5"><i class="fa fa-check"></i><b>6.5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-gradient-descent-algorithm-and-machine-learning"><i class="fa fa-check"></i><b>6.6</b> The Gradient Descent Algorithm and Machine Learning</a><ul>
<li class="chapter" data-level="6.6.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#basic-example"><i class="fa fa-check"></i><b>6.6.1</b> Basic Example</a></li>
<li class="chapter" data-level="6.6.2" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#adaptive-step-size"><i class="fa fa-check"></i><b>6.6.2</b> Adaptive Step-Size</a></li>
<li class="chapter" data-level="6.6.3" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#decreasing-step-size"><i class="fa fa-check"></i><b>6.6.3</b> Decreasing Step-Size</a></li>
<li class="chapter" data-level="6.6.4" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>6.6.4</b> Stochastic Gradient Descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html"><i class="fa fa-check"></i><b>7</b> Newton and Quasi-Newton Methods</a><ul>
<li class="chapter" data-level="7.1" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#the-modified-newton-method"><i class="fa fa-check"></i><b>7.1</b> The Modified Newton Method</a></li>
<li class="chapter" data-level="7.2" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#convergence-of-newtons-method-for-quadratic-functions"><i class="fa fa-check"></i><b>7.2</b> Convergence of Newton’s Method for Quadratic Functions</a></li>
<li class="chapter" data-level="7.3" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#quasi-newton-methods"><i class="fa fa-check"></i><b>7.3</b> Quasi-Newton Methods</a><ul>
<li class="chapter" data-level="7.3.1" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#the-dfp-quasi-newton-method"><i class="fa fa-check"></i><b>7.3.1</b> The DFP Quasi-Newton Method</a></li>
<li class="chapter" data-level="7.3.2" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#exercises-6"><i class="fa fa-check"></i><b>7.3.2</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html"><i class="fa fa-check"></i><b>8</b> Direct Search Methods for Unconstrained Optimisation</a><ul>
<li class="chapter" data-level="8.1" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#random-walk-method"><i class="fa fa-check"></i><b>8.1</b> Random Walk Method</a></li>
<li class="chapter" data-level="8.2" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#downhill-simplex-method-of-nelder-and-mead"><i class="fa fa-check"></i><b>8.2</b> Downhill Simplex Method of Nelder and Mead</a></li>
<li class="chapter" data-level="8.3" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#rosenbrock-function-example"><i class="fa fa-check"></i><b>8.3</b> Rosenbrock Function Example</a><ul>
<li class="chapter" data-level="8.3.1" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#exercises-7"><i class="fa fa-check"></i><b>8.3.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lagrangian-multipliers-for-constraint-optimisation.html"><a href="lagrangian-multipliers-for-constraint-optimisation.html"><i class="fa fa-check"></i><b>9</b> Lagrangian Multipliers for Constraint Optimisation</a><ul>
<li class="chapter" data-level="9.0.1" data-path="lagrangian-multipliers-for-constraint-optimisation.html"><a href="lagrangian-multipliers-for-constraint-optimisation.html#example-12"><i class="fa fa-check"></i><b>9.0.1</b> Example</a></li>
<li class="chapter" data-level="9.0.2" data-path="lagrangian-multipliers-for-constraint-optimisation.html"><a href="lagrangian-multipliers-for-constraint-optimisation.html#exercises-8"><i class="fa fa-check"></i><b>9.0.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">University of the Witwatersrand</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimisation II APPM2007</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="direct-search-methods-for-unconstrained-optimisation" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Direct Search Methods for Unconstrained Optimisation</h1>
<p>Direct search methods, unlike the Descent methods discussed in earlier Chapters do not require the derivatives of
the function. The Direct search methods require only the objective function values when finding minima and are often known as <em>zeroth-order methods</em> since they use the zeroth-order derivatives of the function. We will consider two <strong>Direct Methods</strong> in this course. Namely, the <strong>Random Walk Method</strong> and the <strong>Downhill Simplex Method</strong>.</p>
<div id="random-walk-method" class="section level2">
<h2><span class="header-section-number">8.1</span> Random Walk Method</h2>
<p>The <em>random walk method</em> is based on generating a sequence of improved approximations to a minimum, where each approximation is derived from the previous approximation. Therefore, <span class="math inline">\(\mathbf{x}_i\)</span> is the approximation to the minimum obtained in the <span class="math inline">\((i - 1)\)</span>th iteration, yielding the relation:
<span class="math display">\[
\mathbf{x}_{i + 1} = \mathbf{x}_i + \lambda \mathbf{u}_i,
\]</span>
where <span class="math inline">\(\lambda\)</span> is some scalar step length and <span class="math inline">\(\mathbf{u}_i\)</span> some random unit vector generated at the <span class="math inline">\(i\)</span>th stage.</p>
<p>We can describe the algorithm as follows:</p>
<ol style="list-style-type: decimal">
<li>Start with an initial point <span class="math inline">\(\mathbf{x}_1\)</span>, a sufficiently large initial step length <span class="math inline">\(\lambda\)</span>, a minimum allowable step length <span class="math inline">\(\epsilon\)</span>, and a maximum permissible number of iterations <span class="math inline">\(N\)</span>.</li>
<li>Find the function value <span class="math inline">\(f_1 = f(\mathbf{x}_1)\)</span>.</li>
<li>Set the iteration number, <span class="math inline">\(i\)</span>, to 1</li>
<li>Generate a set of <span class="math inline">\(n\)</span> random numbers, <span class="math inline">\(r_1, \ldots, r_n\)</span>, each lying in the interval <span class="math inline">\([-1, 1]\)</span> and formulate the unit vector <span class="math inline">\(\mathbf{u}\)</span> as:
<span class="math display">\[
\mathbf{u} = \dfrac{1}{(r_1^2 + r_2^2 + \ldots + r_n^2)^{1/2}}\begin{bmatrix} r_1 \\ r_2 \\ \vdots \\ r_n \end{bmatrix}.
\]</span>
To avoid bias in the calculation, we only accept the vector if the length of:
<span class="math display">\[\dfrac{1}{(r_1^2 + r_2^2 + \ldots + r_n^2)^{1/2}}\ \text{is}\ \leq 1.\]</span></li>
<li>Compute the new vector and the corresponding function value <span class="math inline">\(\mathbf{x} = \mathbf{x}_1 + \lambda \mathbf{u}\)</span> and <span class="math inline">\(f = f(\mathbf{x})\)</span>.</li>
<li>If <span class="math inline">\(f &lt; f_1\)</span>, then set the new values of <span class="math inline">\(\mathbf{x}_1 = \mathbf{x}\)</span> and <span class="math inline">\(f_1 = f\)</span> and go to step 3, else continue to 7.</li>
<li>If <span class="math inline">\(i \leq N\)</span>, set the new iteration to <span class="math inline">\(i + 1\)</span> and go to step 4. Otherwise, if <span class="math inline">\(i &gt; N\)</span>, go to step 8.</li>
<li>Compute new, reduced, step length as <span class="math inline">\(\lambda = \lambda /2\)</span>. If new step length is smaller than or equal to <span class="math inline">\(\epsilon\)</span>, then go to step 9, else go to step 4.</li>
<li>Stop the procedure by taking <span class="math inline">\(\mathbf{x}_\text{opt} = \mathbf{x}_1\)</span> and <span class="math inline">\(f_\text{opt} = f_1\)</span>.</li>
</ol>
<div id="example-11" class="section level4">
<h4><span class="header-section-number">8.1.0.1</span> Example</h4>
<p>Minimise <span class="math inline">\(f(x_1, x_2) = x_1 - x_2 + 2x_1^2 + 2x_1x_2 + x_2^2\)</span> using the random walk method. Begin with the initial point <span class="math inline">\(x_0 = [0, 0]\)</span> and a starting step length of <span class="math inline">\(\lambda = 1\)</span>. Use <span class="math inline">\(\epsilon\)</span> = 0.05 and iteration limit <span class="math inline">\(N = 100\)</span></p>
<pre class="sourceCode python"><code class="sourceCode python">f   <span class="op">=</span> <span class="kw">lambda</span> x1, x2: x1 <span class="op">-</span> x2 <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>x1<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>x1<span class="op">*</span>x2 <span class="op">+</span> x2<span class="op">**</span><span class="dv">2</span>
x0  <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>])
lam <span class="op">=</span> <span class="dv">1</span>
eps <span class="op">=</span> <span class="fl">0.05</span>
n   <span class="op">=</span> <span class="dv">2</span> 
N  <span class="op">=</span> <span class="dv">100</span>
<span class="bu">print</span>(random_walk(f, x0, lam, eps, n, N))</code></pre>
<pre><code>## (array([-0.99950141,  1.49937139]), -1.249999734503782)</code></pre>
<p>Let us plot the function to see if our answer makes sense:</p>
<p><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-60-1.png" /><!-- -->
<img src="OptimisationII_notes_files/figure-html/unnamed-chunk-61-1.png" /><!-- --></p>
</div>
</div>
<div id="downhill-simplex-method-of-nelder-and-mead" class="section level2">
<h2><span class="header-section-number">8.2</span> Downhill Simplex Method of Nelder and Mead</h2>
<p>A direct search method for the unconstrained optimisation problem is the Downhill simplex method developed by Nelder and Mead (1965). It does not make an assumption on the cost function to minimise. Importantly, the function in question does not need to satisfy any condition of differentiability unlike other methods, i.e. it is a zero order method. It makes use of simplices, or polytopes in given dimension <span class="math inline">\(n + 1\)</span>. For example, in 2 dimensions, the simplex is a polytope of 3 vertices (triangle). In 3 dimensional space it forms a tetrahedron.</p>
The method starts from an initial simplex. Subsequent steps of the method consist of updating the simplex where it defines:

<p>The movement of the simplex is achieved by using three operations, known as reflection, contraction and expansion.</p>
<p>These can be seen in the Figures below:</p>
<center>
<div style="float:center" markdown="1">
<div class="figure">
<img src="Figures/nelder-mead-expansion.png" alt="Here we have reflection and expansion." />
<p class="caption">Here we have reflection and expansion.</p>
</div>
</div>
</center>
<center>
<div style="float:center" markdown="1">
<div class="figure">
<img src="Figures/nelder-mead-contraction.png" alt="Here we have contraction." />
<p class="caption">Here we have contraction.</p>
</div>
</div>
</center>
<center>
<div style="float:center" markdown="1">
<div class="figure">
<img src="Figures/nelder-mead-mult-contraction.png" alt="Here we have multiple contraction." />
<p class="caption">Here we have multiple contraction.</p>
</div>
</div>
</center>
<p>A common practice to generate the initial remaining simplex vertices is to make use of <span class="math inline">\(\mathbf{x}_0 + \mathbf{e}_i b\)</span>, where <span class="math inline">\(\mathbf{e}_i\)</span> is the unit vector in the direction of the <span class="math inline">\(x_i\)</span> coordinate and <span class="math inline">\(b\)</span> an edge length. Assume a value of 0.1 for <span class="math inline">\(b\)</span>.</p>
<p>Let <span class="math inline">\(y = f(\mathbf{x})\)</span> and <span class="math inline">\(y^h = f(\mathbf{x}^h)\)</span> then the algorithm suggested by Nelder and Mead is as follows:</p>
<center>
<div style="float:center" markdown="1">
<p><img src="Figures/nelder-mead-algo.png" /></p>
</div>
</center>
<p>The typical values for the above factors are <span class="math inline">\(\alpha = 1\)</span>, <span class="math inline">\(\gamma = 2\)</span> and <span class="math inline">\(\beta = 0.5\)</span>. The stopping criteria to use is defined by:
<span class="math display">\[\begin{eqnarray}
\sqrt{\dfrac{1}{n+1}\sum_{i=0}^{n}\left(f(x_i) - \overline{f(x_i)}\right)^2} \leq \epsilon
\end{eqnarray}\]</span></p>
Here is a gif of the method in action:
<center>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Nelder-Mead_Rosenbrock.gif/320px-Nelder-Mead_Rosenbrock.gif"></p>
</center>
</div>
<div id="rosenbrock-function-example" class="section level2">
<h2><span class="header-section-number">8.3</span> Rosenbrock Function Example</h2>
<p>Recall the Rosenbrock function:
<span class="math display">\[\begin{eqnarray}
f(\mathbf{x}) = (1 - x_1)^2 + 10(x_2 - x_1^2)^2,
\end{eqnarray}\]</span></p>
<p>Applying the downhill simplex method on the above equation gives:</p>
<center>
<div style="float:center" markdown="1">
<div class="figure">
<img src="Figures/Sample_case_01.png" alt="Application of Downhill Simplex on Rosenbrock Function - 3D." style="width:80.0%" />
<p class="caption">Application of Downhill Simplex on Rosenbrock Function - 3D.</p>
</div>
</div>
</center>
<p>With the 2D contours looking as follows:</p>
<center>
<div style="float:center" markdown="1">
<div class="figure">
<img src="Figures/Sample_case_02.png" alt="Application of Downhill Simplex on Rosenbrock Function - 2D." style="width:80.0%" />
<p class="caption">Application of Downhill Simplex on Rosenbrock Function - 2D.</p>
</div>
</div>
</center>
<hr />
<div id="exercises-7" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Exercises</h3>
<ol style="list-style-type: decimal">
<li>Apply the above two strategies to the all the multivariate function introduced in earlier chapters and achieve their respective minima.</li>
</ol>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="newton-and-quasi-newton-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lagrangian-multipliers-for-constraint-optimisation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-direct_search_methods_for_unconstrained_optimisation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["OptimisationII_notes.pdf", "OptimisationII_notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
