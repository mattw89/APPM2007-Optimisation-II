<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Multivariate Unconstrained Optimisation | Optimisation II APPM2007</title>
  <meta name="description" content="Course notes for Optimisation II at the University of the Witwatersrand" />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Multivariate Unconstrained Optimisation | Optimisation II APPM2007" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="wits_high_def-min.png" />
  <meta property="og:description" content="Course notes for Optimisation II at the University of the Witwatersrand" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Multivariate Unconstrained Optimisation | Optimisation II APPM2007" />
  
  <meta name="twitter:description" content="Course notes for Optimisation II at the University of the Witwatersrand" />
  <meta name="twitter:image" content="wits_high_def-min.png" />

<meta name="author" content="Dr Matthew Woolway" />


<meta name="date" content="2019-06-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="numerical-optimisation-of-univariate-functions.html">
<link rel="next" href="gradient-methods-for-unconstrained-optimisation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Optimisation II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course Outline</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-structure-and-details"><i class="fa fa-check"></i>Course Structure and Details</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-assessment"><i class="fa fa-check"></i>Course Assessment</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#course-topics"><i class="fa fa-check"></i>Course Topics</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#hardware-requirements"><i class="fa fa-check"></i>Hardware Requirements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html"><i class="fa fa-check"></i><b>1</b> Definition and General Concepts</a><ul>
<li class="chapter" data-level="1.1" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#nonlinear-optimisation"><i class="fa fa-check"></i><b>1.1</b> Nonlinear Optimisation</a></li>
<li class="chapter" data-level="1.2" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#general-statement-of-a-optimisation-problem"><i class="fa fa-check"></i><b>1.2</b> General Statement of a Optimisation Problem</a></li>
<li class="chapter" data-level="1.3" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#important-optimisation-concepts"><i class="fa fa-check"></i><b>1.3</b> Important Optimisation Concepts</a><ul>
<li class="chapter" data-level="1.3.1" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#definitions"><i class="fa fa-check"></i><b>1.3.1</b> Definitions</a></li>
<li class="chapter" data-level="1.3.2" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#convexity"><i class="fa fa-check"></i><b>1.3.2</b> Convexity</a></li>
<li class="chapter" data-level="1.3.3" data-path="definition-and-general-concepts.html"><a href="definition-and-general-concepts.html#exercises"><i class="fa fa-check"></i><b>1.3.3</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html"><i class="fa fa-check"></i><b>2</b> One Dimensional Unconstrained and Bound Constrained Problems</a><ul>
<li class="chapter" data-level="2.1" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#unimodal-and-multimodal"><i class="fa fa-check"></i><b>2.1</b> Unimodal and Multimodal</a></li>
<li class="chapter" data-level="2.2" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#convex-functions"><i class="fa fa-check"></i><b>2.2</b> Convex Functions</a></li>
<li class="chapter" data-level="2.3" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#global-extrema"><i class="fa fa-check"></i><b>2.3</b> Global Extrema</a></li>
<li class="chapter" data-level="2.4" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#necessary-and-sufficient-conditions"><i class="fa fa-check"></i><b>2.4</b> Necessary and Sufficient Conditions</a><ul>
<li class="chapter" data-level="2.4.1" data-path="one-dimensional-unconstrained-and-bound-constrained-problems.html"><a href="one-dimensional-unconstrained-and-bound-constrained-problems.html#exercises-1"><i class="fa fa-check"></i><b>2.4.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html"><i class="fa fa-check"></i><b>3</b> Numerical Solutions to Nonlinear Equations</a><ul>
<li class="chapter" data-level="3.1" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#newtons-method"><i class="fa fa-check"></i><b>3.1</b> Newton’s Method</a><ul>
<li class="chapter" data-level="3.1.1" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#advantages-and-disadvantages-of-newtons-method"><i class="fa fa-check"></i><b>3.1.1</b> Advantages and Disadvantages of Newton’s Method</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#secant-method"><i class="fa fa-check"></i><b>3.2</b> Secant Method</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numerical-solutions-to-nonlinear-equations.html"><a href="numerical-solutions-to-nonlinear-equations.html#exercises-2"><i class="fa fa-check"></i><b>3.2.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html"><i class="fa fa-check"></i><b>4</b> Numerical Optimisation of Univariate Functions</a><ul>
<li class="chapter" data-level="4.1" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#techniques-using-function-evaluations"><i class="fa fa-check"></i><b>4.1</b> Techniques Using Function Evaluations</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#bisection-method"><i class="fa fa-check"></i><b>4.1.1</b> Bisection Method</a></li>
<li class="chapter" data-level="4.1.2" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#golden-search-method"><i class="fa fa-check"></i><b>4.1.2</b> Golden Search Method</a></li>
<li class="chapter" data-level="4.1.3" data-path="numerical-optimisation-of-univariate-functions.html"><a href="numerical-optimisation-of-univariate-functions.html#exercises-3"><i class="fa fa-check"></i><b>4.1.3</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html"><i class="fa fa-check"></i><b>5</b> Multivariate Unconstrained Optimisation</a><ul>
<li class="chapter" data-level="5.1" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#terminology-for-functions-of-several-variables"><i class="fa fa-check"></i><b>5.1</b> Terminology for Functions of Several Variables</a></li>
<li class="chapter" data-level="5.2" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#a-line-in-a-particular-direction-in-the-context-of-optimisation"><i class="fa fa-check"></i><b>5.2</b> A Line in a Particular Direction in the Context of Optimisation</a></li>
<li class="chapter" data-level="5.3" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#taylor-series-for-multivariate-function"><i class="fa fa-check"></i><b>5.3</b> Taylor Series for Multivariate Function</a></li>
<li class="chapter" data-level="5.4" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#quadratic-forms"><i class="fa fa-check"></i><b>5.4</b> Quadratic Forms</a></li>
<li class="chapter" data-level="5.5" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#stationary-points"><i class="fa fa-check"></i><b>5.5</b> Stationary Points</a><ul>
<li class="chapter" data-level="5.5.1" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#tests-for-positive-definiteness"><i class="fa fa-check"></i><b>5.5.1</b> Tests for Positive Definiteness</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#necessary-and-sufficient-conditions-1"><i class="fa fa-check"></i><b>5.6</b> Necessary and Sufficient Conditions</a><ul>
<li class="chapter" data-level="5.6.1" data-path="multivariate-unconstrained-optimisation.html"><a href="multivariate-unconstrained-optimisation.html#exercises-4"><i class="fa fa-check"></i><b>5.6.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html"><i class="fa fa-check"></i><b>6</b> Gradient Methods for Unconstrained Optimisation</a><ul>
<li class="chapter" data-level="6.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#general-line-search-techniques-used-in-unconstrained-multivariate-minimisation"><i class="fa fa-check"></i><b>6.1</b> General Line Search Techniques used in Unconstrained Multivariate Minimisation</a><ul>
<li class="chapter" data-level="6.1.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#challenges-in-computing-step-length-alphak"><i class="fa fa-check"></i><b>6.1.1</b> Challenges in Computing Step Length <span class="math inline">\(\alpha^k\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#exact-and-inexact-line-search"><i class="fa fa-check"></i><b>6.2</b> Exact and Inexact Line Search</a><ul>
<li class="chapter" data-level="6.2.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#algorithmic-structure"><i class="fa fa-check"></i><b>6.2.1</b> Algorithmic Structure</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-descent-condition"><i class="fa fa-check"></i><b>6.3</b> The Descent Condition</a></li>
<li class="chapter" data-level="6.4" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-direction-of-greatest-reduction"><i class="fa fa-check"></i><b>6.4</b> The Direction of Greatest Reduction</a></li>
<li class="chapter" data-level="6.5" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-method-of-steepest-descent"><i class="fa fa-check"></i><b>6.5</b> The Method of Steepest Descent</a><ul>
<li class="chapter" data-level="6.5.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#steepest-descent-algorithm"><i class="fa fa-check"></i><b>6.5.1</b> Steepest Descent Algorithm</a></li>
<li class="chapter" data-level="6.5.2" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#convergence-criteria"><i class="fa fa-check"></i><b>6.5.2</b> Convergence Criteria</a></li>
<li class="chapter" data-level="6.5.3" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#inexact-line-search"><i class="fa fa-check"></i><b>6.5.3</b> Inexact Line Search</a></li>
<li class="chapter" data-level="6.5.4" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#exercises-5"><i class="fa fa-check"></i><b>6.5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#the-gradient-descent-algorithm-and-machine-learning"><i class="fa fa-check"></i><b>6.6</b> The Gradient Descent Algorithm and Machine Learning</a><ul>
<li class="chapter" data-level="6.6.1" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#basic-example"><i class="fa fa-check"></i><b>6.6.1</b> Basic Example</a></li>
<li class="chapter" data-level="6.6.2" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#adaptive-step-size"><i class="fa fa-check"></i><b>6.6.2</b> Adaptive Step-Size</a></li>
<li class="chapter" data-level="6.6.3" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#decreasing-step-size"><i class="fa fa-check"></i><b>6.6.3</b> Decreasing Step-Size</a></li>
<li class="chapter" data-level="6.6.4" data-path="gradient-methods-for-unconstrained-optimisation.html"><a href="gradient-methods-for-unconstrained-optimisation.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>6.6.4</b> Stochastic Gradient Descent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html"><i class="fa fa-check"></i><b>7</b> Newton and Quasi-Newton Methods</a><ul>
<li class="chapter" data-level="7.1" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#the-modified-newton-method"><i class="fa fa-check"></i><b>7.1</b> The Modified Newton Method</a></li>
<li class="chapter" data-level="7.2" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#convergence-of-newtons-method-for-quadratic-functions"><i class="fa fa-check"></i><b>7.2</b> Convergence of Newton’s Method for Quadratic Functions</a></li>
<li class="chapter" data-level="7.3" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#quasi-newton-methods"><i class="fa fa-check"></i><b>7.3</b> Quasi-Newton Methods</a><ul>
<li class="chapter" data-level="7.3.1" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#the-dfp-quasi-newton-method"><i class="fa fa-check"></i><b>7.3.1</b> The DFP Quasi-Newton Method</a></li>
<li class="chapter" data-level="7.3.2" data-path="newton-and-quasi-newton-methods.html"><a href="newton-and-quasi-newton-methods.html#exercises-6"><i class="fa fa-check"></i><b>7.3.2</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html"><i class="fa fa-check"></i><b>8</b> Direct Search Methods for Unconstrained Optimisation</a><ul>
<li class="chapter" data-level="8.1" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#random-walk-method"><i class="fa fa-check"></i><b>8.1</b> Random Walk Method</a></li>
<li class="chapter" data-level="8.2" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#downhill-simplex-method-of-nelder-and-mead"><i class="fa fa-check"></i><b>8.2</b> Downhill Simplex Method of Nelder and Mead</a></li>
<li class="chapter" data-level="8.3" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#rosenbrock-function-example"><i class="fa fa-check"></i><b>8.3</b> Rosenbrock Function Example</a><ul>
<li class="chapter" data-level="8.3.1" data-path="direct-search-methods-for-unconstrained-optimisation.html"><a href="direct-search-methods-for-unconstrained-optimisation.html#exercises-7"><i class="fa fa-check"></i><b>8.3.1</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lagrangian-multipliers-for-constraint-optimisation.html"><a href="lagrangian-multipliers-for-constraint-optimisation.html"><i class="fa fa-check"></i><b>9</b> Lagrangian Multipliers for Constraint Optimisation</a><ul>
<li class="chapter" data-level="9.0.1" data-path="lagrangian-multipliers-for-constraint-optimisation.html"><a href="lagrangian-multipliers-for-constraint-optimisation.html#example-12"><i class="fa fa-check"></i><b>9.0.1</b> Example</a></li>
<li class="chapter" data-level="9.0.2" data-path="lagrangian-multipliers-for-constraint-optimisation.html"><a href="lagrangian-multipliers-for-constraint-optimisation.html#exercises-8"><i class="fa fa-check"></i><b>9.0.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">University of the Witwatersrand</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Optimisation II APPM2007</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariate-unconstrained-optimisation" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Multivariate Unconstrained Optimisation</h1>
<p>Unconstrained optimisation is optimisation when we know we do not have to worry about the boundaries of the feasible set.
<span class="math display">\[\begin{equation}
\underset{s.t.\ x \in S}{\text{min}} \ \ \  f(x)
\end{equation}\]</span>
where <span class="math inline">\(S\)</span> is the feasible set. It should then be possible to find local minima and maxima just by looking at the behaviour of the objective function; and indeed sufficient and necessary conditions. In this chapter these conditions will be derived. The idea of a line in a particular direction is important for any unconstrained optimization methods, we discuss this and derive the slope and curvature of the function <span class="math inline">\(f\)</span> at a point on the line.</p>
<div id="terminology-for-functions-of-several-variables" class="section level2">
<h2><span class="header-section-number">5.1</span> Terminology for Functions of Several Variables</h2>
<p>For a function <span class="math inline">\(f(\mathbf{x}) \in \mathbb{R}^n\)</span> there exists, at any point <span class="math inline">\(\mathbf{x}\)</span> a vector of first order partial derivatives, or gradient vector:
<span class="math display">\[\begin{equation}
\nabla f(\mathbf{x}) = \begin{bmatrix} \dfrac{\partial f}{\partial x_1}(\mathbf{x}) \\ \dfrac{\partial f}{\partial x_2}(\mathbf{x}) \\ \vdots \\ \dfrac{\partial f}{\partial x_n}(\mathbf{x}) \end{bmatrix} = \mathbf{g(x)}.
\end{equation}\]</span></p>
<pre class="sourceCode mathematica"><code class="sourceCode mathematica"><span class="fu">f</span>[<span class="dt">x_</span>, <span class="dt">y_</span>] := <span class="fu">x</span>^<span class="dv">2</span>/<span class="dv">4</span> - <span class="dv">2</span> <span class="fu">x</span>^<span class="dv">2</span> <span class="fu">y</span> - <span class="dv">3</span> <span class="fu">x</span> <span class="fu">y</span> + <span class="fu">y</span>^<span class="dv">4</span>/<span class="dv">16</span>
grad[<span class="dt">x_</span>, <span class="dt">y_</span>] := Grad[<span class="fu">f</span>[<span class="fu">x</span>, <span class="fu">y</span>], {<span class="fu">x</span>, <span class="fu">y</span>}]
<span class="fu">normal</span>[<span class="dt">x_</span>, <span class="dt">y_</span>] = <span class="fu">Simplify</span>[grad[<span class="fu">x</span>, <span class="fu">y</span>]/<span class="fu">Sqrt</span>[grad[<span class="fu">x</span>, <span class="fu">y</span>].grad[<span class="fu">x</span>, <span class="fu">y</span>]]]
{
  <span class="fu">Manipulate</span>[
   <span class="fu">ContourPlot</span>[<span class="fu">f</span>[<span class="fu">x</span>, <span class="fu">y</span>], {<span class="fu">x</span>, <span class="dv">-2</span>, <span class="dv">2</span>}, {<span class="fu">y</span>, <span class="dv">-2</span>, <span class="dv">2</span>}, 
    <span class="fu">Epilog</span> -&gt; <span class="fu">Arrow</span>[{pt, pt + <span class="fu">normal</span> @@ pt}], 
    <span class="fu">PerformanceGoal</span> -&gt; &quot;Quality&quot;, <span class="fu">Contours</span> -&gt; <span class="dv">20</span>, 
    <span class="fu">PlotRange</span> -&gt; {{-<span class="dv">2</span>, <span class="dv">2</span>}, {-<span class="dv">2</span>, <span class="dv">2</span>}, {-<span class="dv">30</span>, <span class="dv">30</span>}}, 
    <span class="fu">ImageSize</span> -&gt; <span class="fu">Medium</span>], {{pt, {.<span class="dv">01</span>, <span class="dv">-0</span><span class="fl">.1</span>}}, <span class="fu">Locator</span>}, 
   <span class="fu">FrameLabel</span> -&gt; &quot;Click <span class="fu">a</span> <span class="fu">point</span> to see its normal&quot;, 
   <span class="fu">SaveDefinitions</span> -&gt; <span class="fu">True</span>],
  <span class="fu">Plot3D</span>[<span class="fu">x</span>^<span class="dv">2</span>/<span class="dv">4</span> - <span class="dv">2</span> <span class="fu">x</span>^<span class="dv">2</span> <span class="fu">y</span> - <span class="dv">3</span> <span class="fu">x</span> <span class="fu">y</span> + <span class="fu">y</span>^<span class="dv">4</span>/<span class="dv">16</span>, {<span class="fu">x</span>, <span class="dv">-2</span> , <span class="dv">2</span>}, {<span class="fu">y</span>,  <span class="dv">-2</span>, <span class="dv">2</span>}, 
   <span class="fu">PlotRange</span> -&gt; <span class="fu">Automatic</span>, <span class="fu">ColorFunction</span> -&gt; &quot;DarkRainbow&quot;, 
   <span class="fu">ImageSize</span> -&gt; <span class="fu">Large</span>]
  }</code></pre>
<center>
<div style="float:center" markdown="1">
<div class="figure">
<img src="Figures/grad.png" alt="Mathematica Demo of Gradient." />
<p class="caption">Mathematica Demo of Gradient.</p>
</div>
</div>
</center>
<p>It can be shown that if the function <span class="math inline">\(f(\mathbf{x})\)</span> is smooth, then at the point <span class="math inline">\(\mathbf{x}\)</span> the gradient vector <span class="math inline">\(\nabla f(\mathbf{x})\)</span> (denoted by <span class="math inline">\(g(\mathbf{x})\)</span>) is always perpendicular to the contours (or surfaces of constant function value) and is the <strong>direction of maximum increase</strong> of <span class="math inline">\(f(\mathbf{x})\)</span> as seen in the Figure above. You can copy the <em>Mathematica</em> code to generate the output above. The manipulation construct will allow you to move the point around to see the gradient at different contours.</p>
<p>If <span class="math inline">\(f(\mathbf{x})\)</span> is twice continuously differentiable then at the point <span class="math inline">\(\mathbf{x}\)</span> there exists a matrix of second order partial derivatives called the <strong>Hessian matrix</strong>:
<span class="math display">\[\begin{equation}
\mathbf{H(x)}=\begin{bmatrix}
\dfrac{\partial^2 f}{\partial x_1^2}(\mathbf{x}) &amp; \dfrac{\partial^2 f}{\partial x_1 \partial x_2}(\mathbf{x}) &amp; \ldots &amp; \dfrac{\partial^2 f}{\partial x_1 \partial x_n}(\mathbf{x}) \\ \dfrac{\partial^2 f}{\partial x_2\partial x_1}(\mathbf{x}) &amp; \ddots &amp; &amp; \\
\dfrac{\partial^2 f}{\partial x_n \partial x_1}(\mathbf{x}) &amp; \ldots &amp; &amp; \dfrac{\partial^2 f}{\partial x_n^2}(\mathbf{x}) 
\end{bmatrix}= \nabla^2f(\mathbf{x})
\end{equation}\]</span></p>
<hr />
<div id="example-2" class="section level4">
<h4><span class="header-section-number">5.1.0.1</span> Example</h4>
<p>Let <span class="math inline">\(f(x_1, x_2) = 5x_1 + 8x_2 + x_1x_2 - x_1^2 - 2x_2^2\)</span>. Then:
<span class="math display">\[\begin{equation*}
\nabla f(\mathbf{x}) = \begin{bmatrix} 5 + x_2 - 2x_1 \\ 8 + x_1 - 4x_2\end{bmatrix}, 
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
\nabla^2f(\mathbf{x}) = \begin{bmatrix} -2 &amp; 1 \\ 1 &amp; -4\end{bmatrix}. 
\end{equation*}\]</span></p>
<!---
\begin{definition}(\#def:lineDirection) 
 --->
<div class="alert alert-info">

<div class="definition">
<span id="def:unnamed-chunk-31" class="definition"><strong>Definition 5.1  (Feasible Direction)  </strong></span>A vector <span class="math inline">\(d\in \mathbb{R}^n\)</span>, <span class="math inline">\(d\neq 0\)</span>, is a <strong>feasible direction</strong> at <span class="math inline">\(\mathbf{x}\in S\)</span> if there exists <span class="math inline">\(\alpha_0&gt;0\)</span> such that <span class="math inline">\(x+\alpha_0d\in S\)</span> for all <span class="math inline">\(\alpha\in [0,\alpha_0]\)</span>.
</div>
</div>
<div class="alert alert-info">

<div class="definition">
<p><span id="def:directionDerivative" class="definition"><strong>Definition 5.2  (Directional Derivative)  </strong></span>Let <span class="math inline">\(f:\mathbb{R}^n\to \mathbb{R}\)</span> be a real-valued function and let <span class="math inline">\(d\)</span> be a feasible direction at <span class="math inline">\(\mathbf{x}\in S\)</span>. The <strong>directional derivative</strong> of <span class="math inline">\(f\)</span> in the direction of <span class="math inline">\(d\)</span>, denoted by <span class="math inline">\(d^T\nabla f(\mathbf{x})\)</span>, is given by:
<span class="math display">\[\begin{equation}
\nabla f^Td=\lim_{\alpha\to 0}\dfrac{f(\mathbf{x}+\alpha d)-f(\mathbf{x})}{\alpha}
\end{equation}\]</span>
If <span class="math inline">\(\lVert d\rVert=1\)</span>, then <span class="math inline">\(d^T\nabla f(\mathbf{x})\)</span> is the rate of increase of <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{x}\)</span> in the direction <span class="math inline">\(d\)</span>. To compute the above directional derivative, suppose that <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(d\)</span> are given. Then, <span class="math inline">\(f(\mathbf{x}+\alpha d)\)</span> is a function of <span class="math inline">\(\alpha\)</span>, and:
<span class="math display">\[\begin{equation}
d^T\nabla f(\mathbf{x})=\frac{d}{d\alpha} f(\mathbf{x}+\alpha d)\big\vert_{\alpha=0}.
\end{equation}\]</span></p>
</div>
</div>
</div>
</div>
<div id="a-line-in-a-particular-direction-in-the-context-of-optimisation" class="section level2">
<h2><span class="header-section-number">5.2</span> A Line in a Particular Direction in the Context of Optimisation</h2>
<p>A line is a set of points <span class="math inline">\(\mathbf{x}\)</span> such that:
<span class="math display" id="eq:lineDirection">\[\begin{equation}
\mathbf{x} = \mathbf{x}^\prime + \alpha \mathbf{d},  \ \ \ \forall \ \alpha,\tag{5.1}
\end{equation}\]</span>
where <span class="math inline">\(\mathbf{d}\)</span> and <span class="math inline">\(\mathbf{x}^\prime\)</span> are given. For <span class="math inline">\(\alpha \geq 0\)</span> Equation <a href="multivariate-unconstrained-optimisation.html#eq:lineDirection">(5.1)</a> is a half-line. The point <span class="math inline">\(\mathbf{x}^\prime\)</span> is a fixed point (corresponding to <span class="math inline">\(\alpha = 0\)</span>) along the line, <span class="math inline">\(\mathbf{d}\)</span> is the direction of the line. For instance, if we take the fixed point <span class="math inline">\(\mathbf{x}&#39;\)</span> to be <span class="math inline">\((2,2)^T\)</span> and the direction <span class="math inline">\(\mathbf{d} = (3,1)^T\)</span> then the Figure below shows the line in the direction of <span class="math inline">\(\mathbf{d}\)</span>.</p>
<center>
<div style="float:center" markdown="1">
<div class="figure">
<img src="Figures/lineDirection.png" alt="An Example of a Line in a Particular Direction." />
<p class="caption">An Example of a Line in a Particular Direction.</p>
</div>
</div>
</center>
<p>The vector <span class="math inline">\(\mathbf{d}\)</span> in indicated by the arrow. If we normalise the vector <span class="math inline">\(\mathbf{d}\)</span> so that <span class="math inline">\(\mathbf{d}^T\mathbf{d}=\sum_id_i^2=1\)</span>. This does not change the line, but only the value of <span class="math inline">\(\alpha\)</span> associated with any point along the line. For Example:</p>
<hr />
<pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">from</span> numpy <span class="im">import</span> linalg <span class="im">as</span> LA
d     <span class="op">=</span> np.array([<span class="dv">3</span>, <span class="dv">1</span>])
alpha <span class="op">=</span> LA.norm(d, <span class="dv">2</span>)
<span class="bu">print</span>(d)</code></pre>
<pre><code>## [3 1]</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(alpha)</code></pre>
<pre><code>## 3.1622776601683795</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">norm_d <span class="op">=</span> d<span class="op">/</span>alpha
<span class="bu">print</span>(<span class="st">&#39;The normalised vector d is:&#39;</span>)</code></pre>
<pre><code>## The normalised vector d is:</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(norm_d)</code></pre>
<pre><code>## [0.9486833  0.31622777]</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(<span class="st">&#39;The normalised d^Td gives:f&#39;</span>, np.dot(norm_d,norm_d))</code></pre>
<pre><code>## The normalised d^Td gives:f 0.9999999999999999</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(<span class="st">&#39;So alpha x normalised d returns d:&#39;</span>)</code></pre>
<pre><code>## So alpha x normalised d returns d:</code></pre>
<pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span>(alpha<span class="op">*</span>norm_d)</code></pre>
<pre><code>## [3. 1.]</code></pre>
<p>We now use the gradient and the Hessian of <span class="math inline">\(f(\mathbf{x})\)</span> to derive the derivative of <span class="math inline">\(f(\mathbf{x})\)</span> along a line of any direction. For a fixed line of a given direction like Equation <a href="multivariate-unconstrained-optimisation.html#eq:lineDirection">(5.1)</a> we see that the points on the line is a function of <span class="math inline">\(\alpha\)</span> only. Hence a change in <span class="math inline">\(\alpha\)</span> causes change in all coordinates of <span class="math inline">\(\mathbf{x}(\alpha)\)</span>. The derivative of <span class="math inline">\(f(\mathbf{x})\)</span> with respect to <span class="math inline">\(\alpha:\)</span>
<span class="math display" id="eq:doperator">\[\begin{equation}
\frac{df\left (\mathbf{x} (\alpha)\right)}{d\alpha} = \frac{\partial f(\mathbf{x}(\alpha))}{\partial x_1} \frac {dx_1(\alpha)}{d\alpha} + \frac{\partial f(\mathbf{x}(\alpha))}{\partial x_2}\frac {d x_2(\alpha)}{\partial\alpha} +\cdots +\frac{\partial f(\mathbf{x}(\alpha))}{\partial x_n} \frac{d x_n(\alpha)}{d\alpha} \tag{5.2}
\end{equation}\]</span>
The Equation <a href="multivariate-unconstrained-optimisation.html#eq:doperator">(5.2)</a> represents the derivative of <span class="math inline">\(f(\mathbf{x})\)</span> at any point <span class="math inline">\(\mathbf{x}(\alpha)\)</span> along the line. The operator <span class="math inline">\(\frac{d}{d\alpha}\)</span> can be expressed as:
<span class="math display" id="eq:doperator2">\[\begin{equation}
\frac{d}{d\alpha} = \dfrac{\partial}{\partial x_1}\frac {d x_1}{ d\alpha} + \frac{\partial}{ \partial x_2}\dfrac {d x_2}{d \alpha} + \cdots +\dfrac{\partial}{\partial x_n}\dfrac {dx_n}{d\alpha} = \mathbf{d}^T\nabla \tag{5.3}
\end{equation}\]</span>
The slope of <span class="math inline">\(f(\mathbf{x})\)</span> at <span class="math inline">\(\mathbf{x}(\alpha)\)</span> can be written as:
<span class="math display" id="eq:slope">\[\begin{equation}
{df\over d\alpha} = \mathbf{d}^T\nabla f(\mathbf{x}(\alpha)) = \nabla f (\mathbf{x}(\alpha))^T\mathbf{d}.\tag{5.4}
\end{equation}\]</span>
Likewise, the curvature of <span class="math inline">\(f(\mathbf{x}(\alpha))\)</span> along the line:
<span class="math display">\[\begin{equation}
{d^2 f\over d\alpha^2} = {d\over d\alpha} \left({df(\mathbf{x}(\alpha))\over d\alpha}\right) = \mathbf{d}^T\nabla \left(\nabla f^T \mathbf{d}\right) = \mathbf{d}^T\nabla^2f\mathbf{d},
\end{equation}\]</span>
where <span class="math inline">\(\nabla f\)</span> and <span class="math inline">\(\nabla^2f\)</span> are evaluated at <span class="math inline">\(\mathbf{x}(\alpha)\)</span>. These (slope and curvature) when evaluated at <span class="math inline">\(\alpha\)</span>=0 are respectively known as derivative (also called slope since <span class="math inline">\(f=f(\alpha)\)</span> is now a function of the single variable <span class="math inline">\(\alpha\)</span>) and curvature of <span class="math inline">\(f\)</span> at <span class="math inline">\(x&#39;\)</span> in the direction of <span class="math inline">\(d\)</span>.</p>
<hr />
<div id="example-3" class="section level4">
<h4><span class="header-section-number">5.2.0.1</span> Example</h4>
<p>Let us consider the Rosenbrock’s function:
<span class="math display" id="eq:rosenbrock">\[\begin{equation}
f(\mathbf{x})=100(x_2-x_1^2)^2 + (1-x_1)^2 \tag{5.5}
\end{equation}\]</span>
If <span class="math inline">\(\mathbf{x}^\prime = {0 \choose 0}\)</span> then show that the slope of <span class="math inline">\(f(\mathbf{x})\)</span> along the line generated by <span class="math inline">\(\mathbf{d} = {1 \choose 0}\)</span> is <span class="math inline">\(\mathbf{d}^T\nabla f=-2\)</span> and the curvature is <span class="math inline">\(\mathbf{d}^T G\mathbf{d} = 2\)</span> where <span class="math inline">\(G=\nabla^2 f(\mathbf{x}^\prime).\)</span></p>
<p><strong>Solution:</strong></p>
<p><span class="math display">\[\begin{eqnarray*}
        \bigtriangledown f = \begin{bmatrix}
    -400x_1(x_2 - x_1^2) - 2(1 - x_1)   \\ 
        200(x_2 - x_1^2)
        \end{bmatrix} =
        \begin{bmatrix}
        -2 \\ 
        0
        \end{bmatrix} 
\end{eqnarray*}\]</span>
Therefore <span class="math inline">\(\mathbf{d} \bigtriangledown f = [1\ \ \  0] \times [-2\ \ \  0]^T = -2\)</span>. Next:
<span class="math display">\[\begin{eqnarray*}
            \bigtriangledown^2 f = \begin{bmatrix}
            -400(x_2 - x_1^2) + 800x_1^2 + 2 &amp; -400x_1  \\ 
            -400x_1 &amp; 200
            \end{bmatrix} =
            \begin{bmatrix}
            2 &amp; 0\\ 
            0 &amp; 200
            \end{bmatrix} 
\end{eqnarray*}\]</span>
Thus <span class="math inline">\(\mathbf{d}^T G \mathbf{d} = [1\ \ \ \ 0] \times \begin{bmatrix}  2 &amp; 0\\  0 &amp; 200 \end{bmatrix} \times [1\ \ \ \  0]^T = 2\)</span></p>
<hr />
<blockquote>
<p>These definitions of slope and curvature depend on the size of <span class="math inline">\(\mathbf{d},\)</span> and this ambiguity can be resolved by requiring that <span class="math inline">\(\lVert\mathbf{d}\rVert=1.\)</span> Hence Equation <a href="multivariate-unconstrained-optimisation.html#eq:slope">(5.4)</a> is the directional derivative in the direction of a unit vector <span class="math inline">\(\mathbf{d}\)</span> and this given by <span class="math inline">\(\nabla f(\mathbf{x})\mathbf{d}^T\)</span>. Likewise the curvature along the line in the direction of the unit vector is given by <span class="math inline">\(\mathbf{d}^T\nabla^2f(\mathbf{x})\mathbf{d}.\)</span></p>
</blockquote>
<blockquote>
<p>Since <span class="math inline">\(\mathbf{x}(\alpha)=\mathbf{x}&#39;+\alpha\mathbf{d}\)</span>, at <span class="math inline">\(\alpha=0\)</span> we have <span class="math inline">\(\mathbf{x}(0)=\mathbf{x}&#39;\)</span>. Therefore, the function value <span class="math inline">\(f(\mathbf{x}(0))=f(\mathbf{x}&#39;)\)</span>, the slope at <span class="math inline">\(\alpha=0\)</span> in the direction of <span class="math inline">\(\mathbf{d}\)</span> is <span class="math inline">\(f&#39;(0)=\mathbf{d}^T\nabla f(\mathbf{x}&#39;)\)</span> and the curvature at <span class="math inline">\(\alpha=0\)</span> is <span class="math inline">\(f&#39;&#39;(0)=\mathbf{d}^TG(\mathbf{x}&#39;)\mathbf{d}\)</span>.</p>
</blockquote>
</div>
</div>
<div id="taylor-series-for-multivariate-function" class="section level2">
<h2><span class="header-section-number">5.3</span> Taylor Series for Multivariate Function</h2>
<p>In the context of optimization involving smooth function <span class="math inline">\(f(\mathbf{x})\)</span> the Taylor series is indispensable. Since <span class="math inline">\(\mathbf{x} = \mathbf{x}(\alpha) = \mathbf{x}^\prime + \alpha \mathbf{d}\)</span> for a fixed point <span class="math inline">\(\mathbf{x}&#39;\)</span> and a given direction <span class="math inline">\(\mathbf{d},\)</span> the <span class="math inline">\(f(\mathbf{x})\)</span> at <span class="math inline">\(\mathbf{x}(\alpha)\)</span> becomes a function of the single variable <span class="math inline">\(\alpha.\)</span> Hence, <span class="math inline">\(f(\mathbf{x}) = f(\mathbf{x}(\alpha)) = f(\alpha).\)</span> Therefore, expanding the Taylor series around zero we have:
<span class="math display" id="eq:taylorSeries">\[\begin{equation}
f(\alpha) = f(0+\alpha) = f(0) + \alpha f^\prime(0) + {1\over2}\alpha^2 f^{\prime\prime}(0)+ \cdots \tag{5.6}
\end{equation}\]</span>
But <span class="math inline">\(f(\alpha) = f(\mathbf{x}^\prime+\alpha\mathbf{d})\)</span> is the value of the function
<span class="math inline">\(f(\mathbf{x})\)</span> of many variable along the line <span class="math inline">\(\mathbf{x}(\alpha).\)</span> Hence, we can re-write Equation <a href="multivariate-unconstrained-optimisation.html#eq:taylorSeries">(5.6)</a> as:
<span class="math display">\[\begin{equation}
f(\mathbf{x}^\prime+\alpha \mathbf{d}) = f(\mathbf{x}^\prime) + \alpha \mathbf{d}^T \nabla f(\mathbf{x}^\prime) + {1\over2} \alpha^2 \mathbf{d}^T \left [\nabla^2 f(\mathbf{x}^\prime)\right ] \mathbf{d} + \cdots
\end{equation}\]</span></p>
</div>
<div id="quadratic-forms" class="section level2">
<h2><span class="header-section-number">5.4</span> Quadratic Forms</h2>
<p>The quadratic function in <span class="math inline">\(n\)</span> variables may be written as:
<span class="math display">\[\begin{equation}
f(\mathbf{x}) = \dfrac{1}{2}\mathbf{x}^T\mathbf{A}\mathbf{x} + \mathbf{b}^T\mathbf{x} + c,
\end{equation}\]</span>
where <span class="math inline">\(c \ \in \mathbb{R}\)</span>, <span class="math inline">\(\mathbf{b}\)</span> is a real <span class="math inline">\(n\)</span> vector and <span class="math inline">\(\mathbf{A}\)</span> is a <span class="math inline">\(n \times n\)</span> real matrix that can be chosen in a non-unique manner. It is usually chosen symmetrical in which case it follows that:
<span class="math display">\[\begin{equation}
a_{11}{x_1}^2+2a_{12}x_1x_2+a_{22}{x_2}^2+2a_{13}x_1x_3+\cdots+a_{nn}{x_n}^2= \sum_{i=1}^n\sum_{j=1}^na_{ij}x_ix_j,
\end{equation}\]</span>
and:
<span class="math display">\[\begin{equation}
\nabla f(\mathbf{x}) = \mathbf{A}\mathbf{x} + \mathbf{b}; \ \ \ \mathbf{H(x)} = \mathbf{A}.
\end{equation}\]</span>
The form <span class="math inline">\(A\)</span> is said to be positive definite if <span class="math inline">\(A\geq 0\)</span> for all <span class="math inline">\(\mathbf{x}\)</span> with <span class="math inline">\(A=0\)</span> iff <span class="math inline">\(\mathbf{x}=0\)</span>. The form <span class="math inline">\(A\)</span> is said to be positive semi-definite if <span class="math inline">\(A\geq 0\)</span> for all <span class="math inline">\(\mathbf{x}\)</span>. Similar definitions apply to negative definite and negative semi-definite with the inequalities reversed.</p>
<hr />
<div id="example-4" class="section level4">
<h4><span class="header-section-number">5.4.0.1</span> Example</h4>
<p>Write <span class="math inline">\(A(\mathbf{x})={x_1}^2+5x_1x_2+4{x_2}^2\)</span> in the matrix form.</p>
<p><strong>Solution:</strong> <span class="math inline">\(A(\mathbf{x})=(x_1,x_2)\left (\begin{array}{c} 1 \; \frac{5}{2}\\ \frac{5}{2}\; 4 \end{array} \right)\left( \begin{array}{c} x_1\\ x_2\end{array} \right)\)</span></p>
<hr />
</div>
</div>
<div id="stationary-points" class="section level2">
<h2><span class="header-section-number">5.5</span> Stationary Points</h2>
<p>In the following chapters we will be concerned with gradient based minimization methods. Therefore, we only consider the minimization of smooth functions. We will not consider the non-smooth minima as they do not satisfy the same conditions as smooth minima. We, however, will consider the case of saddle point. Hence, we assume that the first and the second derivative exist.</p>
<p>We can classify definiteness by looking at the eigenvalues of <span class="math inline">\(\nabla^2 f(\mathbf{x})\)</span>. Specifically:</p>
<ul>
<li>If <span class="math inline">\(\nabla^2 f(\mathbf{x}^*)\)</span> is indefinite, i.e. all <span class="math inline">\(\lambda_i\)</span> are mixed sign, then <span class="math inline">\(\mathbf{x}^*\)</span> is a saddle point.</li>
<li>If <span class="math inline">\(\nabla^2 f(\mathbf{x}^*)\)</span> is positive definite, i.e. all <span class="math inline">\(\lambda_i &gt; 0\)</span>, then <span class="math inline">\(\mathbf{x}^*\)</span> is a minimum.</li>
<li>If <span class="math inline">\(\nabla^2 f(\mathbf{x}^*)\)</span> is negative definite, i.e. all <span class="math inline">\(\lambda_i &lt; 0\)</span>, then <span class="math inline">\(\mathbf{x}^*\)</span> is a maximum.</li>
<li>If <span class="math inline">\(\nabla^2 f(\mathbf{x}^*)\)</span> is positive semi-definite, i.e. all <span class="math inline">\(\lambda_i \geq 0\)</span>, then <span class="math inline">\(\mathbf{x}^*\)</span> is a half cylinder.</li>
</ul>
<p>These can be seen in the Figure below:</p>
<p><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-33-1.png" /><!-- --><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-33-2.png" /><!-- --><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-33-3.png" /><!-- --><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-33-4.png" /><!-- --></p>
<p><strong>In summary:</strong></p>
<p>Let <span class="math inline">\(G = \nabla^2 f(\mathbf{x})\)</span>, i.e. the Hessian.</p>
<ul>
<li><span class="math inline">\(G(x)\)</span> is positive semi-definite if <span class="math inline">\(x^TGx \geq 0, \quad \forall x\)</span></li>
<li><span class="math inline">\(G(x)\)</span> is negative semi-definite if <span class="math inline">\(x^TGx \leq 0, \quad \forall x\)</span></li>
<li><span class="math inline">\(G(x)\)</span> is positive definite iff <span class="math inline">\(x^TGx &gt; 0, \quad \forall x \neq 0\)</span></li>
<li><span class="math inline">\(G(x)\)</span> is negative definite iff <span class="math inline">\(x^TGx &lt; 0, \quad \forall x \neq 0\)</span></li>
<li><span class="math inline">\(G(x)\)</span> is indefinite iff <span class="math inline">\(x^TGx\)</span> is mixed negative and positive</li>
</ul>
<p>and:</p>
<ul>
<li><span class="math inline">\(f(x)\)</span> is concave iff <span class="math inline">\(G(x)\)</span> is negative semi-definite</li>
<li><span class="math inline">\(f(x)\)</span> is strictly concave iff <span class="math inline">\(G(x)\)</span> is negative definite</li>
<li><span class="math inline">\(f(x)\)</span> is convex iff <span class="math inline">\(G(x)\)</span> is positive semi-definite</li>
<li><span class="math inline">\(f(x)\)</span> is convex if <span class="math inline">\(G(x)\)</span> is positive definite</li>
</ul>
<div id="tests-for-positive-definiteness" class="section level3">
<h3><span class="header-section-number">5.5.1</span> Tests for Positive Definiteness</h3>
<p>There are a number of ways for us to test for positive or negative definiteness. Namely;</p>
<div id="compute-the-eigenvalues" class="section level4">
<h4><span class="header-section-number">5.5.1.1</span> Compute the Eigenvalues</h4>
<div id="example-5" class="section level5">
<h5><span class="header-section-number">5.5.1.1.1</span> Example</h5>
<p>Classify the stationary points of the function <span class="math display">\[f(\mathbf{x}) = 2x_1^2 + x_1 x_2^2 + x_2^2.\]</span></p>
<p><strong>Solution:</strong></p>
<p>The stationary points are the solutions of
<span class="math display">\[\begin{eqnarray*}
\dfrac{\partial f}{\partial x_1}&amp;=&amp;4x_1+{x_2}^2=0\\
\dfrac{\partial f}{\partial x_2}&amp;=&amp;2x_1x_2+2x_2=0
\end{eqnarray*}\]</span>
which gives <span class="math inline">\(\mathbf{x}_1=(0,0)^T\, , \mathbf{x}_2=(-1,2)^T\ \ \ \ \text{and}\ \ \ \ \mathbf{x}_3=(-1,-2)^T.\)</span> The Hessian matrix is:
<span class="math display">\[
G=\left (
\begin{array}{cc}
4&amp;2x_2\\
2x_2&amp;2x_1+2\\
\end{array}\right )
\]</span>
Thus:
<span class="math display">\[
G_1=\left (
\begin{array}{cc}
4&amp;0\\
0&amp;2
\end{array}\right )
\]</span>
The eigenvalues are the solution of <span class="math display">\[(4-\lambda)(2-\lambda)=0\]</span> which gives <span class="math inline">\(\lambda=4,2\)</span>. Thus <span class="math inline">\(\mathbf{x}_1\)</span> correspond to a minimum. Similarly:
<span class="math display">\[
G_2=\left (
\begin{array}{cc}
4&amp;4\\
4&amp;0
\end{array}\right )
\]</span>
has eigenvalues:
<span class="math display">\[\lambda=2+\sqrt{20},\, 2-\sqrt{20}\]</span>
Thus
<span class="math inline">\(\mathbf{x}_2\)</span> correspond to a saddle point. Finally
<span class="math display">\[
G_3=\left (
\begin{array}{cc}
4&amp;-4\\
-4&amp;0
\end{array}\right )
\]</span>
has the same eigenvalues as <span class="math inline">\(G_2\)</span> and therefore <span class="math inline">\(\mathbf{x}_3\)</span> corresponds to a saddle point.</p>
<p><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-34-1.png" /><!-- --><img src="OptimisationII_notes_files/figure-html/unnamed-chunk-34-2.png" /><!-- --></p>
</div>
</div>
<div id="principle-minors" class="section level4">
<h4><span class="header-section-number">5.5.1.2</span> Principle Minors</h4>
<p>From the Hessian we can compute the determinant of all subminors. If these are all greater than zero, then the Hessian is positive definite. Utilising the example above. If:
<span class="math display">\[
G_1=\left (
\begin{array}{cc}
4&amp;0\\
0&amp;2
\end{array}\right )
\]</span>
Then the first subminor is just det<span class="math inline">\(\lvert 4 \rvert\)</span> which is <span class="math inline">\(&gt; 0\)</span>.
The second and final subminor is the entire matrix, so:
<span class="math display">\[
\text{det}\left \lvert \begin{array}{cc} 4 &amp; 0 \\ 0 &amp; 2 \end{array} \right \rvert = 8 - 0 &gt; 0.
\]</span>
Therefore <span class="math inline">\(G_1\)</span> is positive definite. <span class="math inline">\(G_2\)</span> and <span class="math inline">\(G_3\)</span> are dealt with similarly. However, to prove negative definiteness we need to prove <span class="math inline">\((-1)^k D_k &gt; 0\)</span>, where <span class="math inline">\(D\)</span> is the determinant of the <span class="math inline">\(k\)</span>-th principle minor.</p>
<p>This approach would be preferable when dealing with the case of large matrices.</p>
</div>
</div>
</div>
<div id="necessary-and-sufficient-conditions-1" class="section level2">
<h2><span class="header-section-number">5.6</span> Necessary and Sufficient Conditions</h2>
<div class="alert alert-info">

<div class="theorem">
<span id="thm:unnamed-chunk-35" class="theorem"><strong>Theorem 5.1  (First Order Necessary Condition (FONC) for Local Maxima/Minima)  </strong></span>If <span class="math inline">\(f(\mathbf{x})\)</span> has continuous first partial derivatives at all points of <span class="math inline">\(S\subset R^n\)</span> and if <span class="math inline">\(\mathbf{x}^*\)</span> is an interior point of the feasible set <span class="math inline">\(S\)</span> then <span class="math inline">\(\mathbf{x}^*\)</span> is a local minimum or maximum of <span class="math inline">\(f(\mathbf{x})\)</span>:
<span class="math display">\[\begin{equation}
\nabla f(x^*)=0.
\end{equation}\]</span>
Alternatively, if <span class="math inline">\(\mathbf{x}^*\in S\)</span> is a local minimum or maximum, then at the point <span class="math inline">\(\mathbf{x}^*\)</span>:
<span class="math display" id="eq:multFonc">\[\begin{equation}
{\partial f(x^*)\over \partial x_i} = 0; \quad i=1,2,...n.\tag{5.7}
\end{equation}\]</span>
</div>
</div>
<div class="alert alert-info">

<div class="theorem">
<span id="thm:unnamed-chunk-36" class="theorem"><strong>Theorem 5.2  (Second Order Necessary Condition (SONC) for Local Maxima/Minima)  </strong></span>Let <span class="math inline">\(f\)</span> be twice continuously differentiable on the feasible set <span class="math inline">\(S\)</span>, <span class="math inline">\(\mathbf{x}^*\)</span> is a local minimiser of <span class="math inline">\(f(\mathbf{x})\)</span>, and <span class="math inline">\(\mathbf{d}\)</span> is a feasible direction at <span class="math inline">\(\mathbf{x}^*\)</span>. If <span class="math inline">\(\mathbf{d}^T\nabla f(\mathbf{x}^*)=0\)</span>, then:
<span class="math display">\[\begin{equation}
\mathbf{d}^T\nabla^2 f(\mathbf{x}^*)\mathbf{d}\geq 0.
\end{equation}\]</span>
</div>
</div>
<div class="alert alert-info">

<div class="theorem">
<span id="thm:unnamed-chunk-37" class="theorem"><strong>Theorem 5.3  (Second Order Sufficient Condition (SOSC) for Strong Local Maxima/Minima)  </strong></span>Let <span class="math inline">\(\mathbf{x}^*\)</span> be an interior of <span class="math inline">\(S\)</span>. If <span class="math inline">\(\mathbf{x}^*\)</span> is a local minimiser of <span class="math inline">\(f(\mathbf{x})\)</span> then (i) <span class="math inline">\(\nabla f(\mathbf{x}^*)=0\)</span> and (ii) <span class="math inline">\(\mathbf{d}^T\nabla^2 f(\mathbf{x}^*)\mathbf{d}&gt; 0\)</span>. That is the hessian is positive definite.
</div>
</div>
<hr />
<div id="example-6" class="section level4">
<h4><span class="header-section-number">5.6.0.1</span> Example</h4>
<p>Let <span class="math inline">\(f(\mathbf{x})=x_1^2+x_2^2\)</span>. Show that <span class="math inline">\(\mathbf{x}=(0,0)^T\)</span> satisfies the FONC, the SONC and SOSC hence <span class="math inline">\((0,0)^T\)</span> is a strict local minimiser. We see that <span class="math inline">\(\nabla f(\mathbf{x})=(2x_1,2x_2)=0\)</span> if and only if <span class="math inline">\(x_1=x_2=0\)</span>. It also can be easily shown that for all <span class="math inline">\(\mathbf{d}\neq 0\)</span>, <span class="math inline">\(\mathbf{d}^T\nabla^2 f(x)\mathbf{d}=2d_1^2+2d_2^2&gt;0\)</span>. Hence <span class="math inline">\(\nabla^2 f(\mathbf{x})\)</span> is positive definite.</p>
<hr />
</div>
<div id="example-7" class="section level4">
<h4><span class="header-section-number">5.6.0.2</span> Example</h4>
<p><span class="math display">\[f(x_1,x_2) = x_1^4 + x^4_2\]</span>
<span class="math inline">\(\nabla f(\mathbf{x}) = \begin{pmatrix} 4x^3_1 \\ 4x^3_2 \end{pmatrix}\)</span>. The only stationary point is <span class="math inline">\((0\ 0)^T.\)</span> Now the Hessian <span class="math inline">\(\nabla^2f=\begin{pmatrix} 12x^2_1 &amp; 0 \\ 0 &amp; 12x^2_2 \end{pmatrix}.\)</span> At the origin the Hessian is <span class="math inline">\(\begin{pmatrix} 0&amp; 0 \\ 0 &amp; 0\end{pmatrix}\)</span> and so there is no prediction of the minimum from the test although it is easy to see that the origin is a minimum.</p>
<hr />
</div>
<div id="example-8" class="section level4">
<h4><span class="header-section-number">5.6.0.3</span> Example</h4>
<p><span class="math display">\[f(x_1,x_2) = {1\over 2c} \left({x^2_1\over a^2} + {x^2_2\over b^2}\right),\]</span>
where <span class="math inline">\(a,b,\)</span> and <span class="math inline">\(c\)</span> are constants. <span class="math inline">\(\nabla f(\mathbf{x}) = \begin{pmatrix} {x_1\over ca^2} \\ {-x_2\over cb^2}\end{pmatrix}.\)</span> So
the only stationary point is <span class="math inline">\((0 \ 0)^T.\)</span> The Hessian is <span class="math inline">\({\nabla}^2 f(\mathbf{x}) = \begin{pmatrix} {1\over ca^2} &amp; 0 \\ 0 &amp; - {1\over cb^2}\end{pmatrix}.\)</span> This is clearly indefinite and hence <span class="math inline">\((0\ 0)^T\)</span> is a saddle point.</p>
<hr />
<p>Thus in summary, the <em>necessary and sufficent condition</em> for <span class="math inline">\(\mathbf{x}^*\)</span> to be a <strong>strong</strong> local minimum are:</p>
<ul>
<li><span class="math inline">\(\nabla f(\mathbf{x}^*)=0\)</span></li>
<li>Hessian is positive definite</li>
</ul>
<hr />
</div>
<div id="exercises-4" class="section level3">
<h3><span class="header-section-number">5.6.1</span> Exercises</h3>
<ol style="list-style-type: decimal">
<li>Find the gradient vectors of the following functions (where <span class="math inline">\(\mathbf{x}\in \mathbb{R}^n\)</span>): 
<ul>
<li><span class="math inline">\(f(\mathbf{x})=\mathbf{c}^T\mathbf{x},\quad \mathbf{c}\in \mathbb{R}^n\)</span></li>
<li><span class="math inline">\(f(\mathbf{x})=\frac{1}{2}\mathbf{x}^T\mathbf{x}\)</span></li>
<li><span class="math inline">\(f(\mathbf{x})=\frac{1}{2}\mathbf{x}^TG\mathbf{x}\)</span> where <span class="math inline">\(G\)</span> is symmetric</li>
</ul></li>
<li>Find the slope and the curvature of the following functions.
<ul>
<li><span class="math inline">\(f(\mathbf{x}) = 100 (x_2 - x_1^2) + (1-x_1)^2\)</span> at <span class="math inline">\((0,0)^T\)</span> in the direction of <span class="math inline">\((1,0)^T\)</span>.</li>
<li><span class="math inline">\(f(\mathbf{x}) = x^2_1 - 2x_1 + 3x_1 x_2^2 + 4x_2^3\)</span> at <span class="math inline">\((-1,1)^T\)</span> along <span class="math inline">\((-1,0)^T.\)</span></li>
</ul></li>
<li>Use the <em>necessary condition</em> of optimality to determine the optimiser of the following function <span class="math display">\[f(x_1,x_2)=(x_1-1)^2+(x_2-1)^2+x_1x_2\]</span></li>
<li>Prove that for a general quadratic function <span class="math inline">\(f(\mathbf{x})=c+\mathbf{b}^T\mathbf{x}+\frac{1}{2}\mathbf{x}^TG\mathbf{x}\)</span>, the Hessian <span class="math inline">\(G\)</span> of <span class="math inline">\(f\)</span> maps differences in position into differences in gradient, i.e., <span class="math inline">\(\mathbf{g}^1-\mathbf{g}^2=G(\mathbf{x}^1-\mathbf{x}^2)\)</span>.</li>
<li>For the following functions, find the points where the gradients vanish, and investigate which of these are local minima, maxima or saddle.
<ul>
<li><span class="math inline">\(f(x_1, x_2) = x_1(1+x_1) + x_2 (1+x_2) - 1.\)</span></li>
<li><span class="math inline">\(f(x_1, x_2) = {x_1}^2 + x_1x_2+{x_2}^2.\)</span></li>
</ul></li>
<li>Consider the function <span class="math inline">\(f : R^2\to R\)</span> determined by
<span class="math display">\[f(x)=x^T\left [ \begin{array}{cc} 1 &amp; 2\\ 4 &amp; 8 \end{array}    \right]x+x^T \left [ \begin{array}{c} 3\\ 4 \end{array}    \right] +6\]</span>
<ul>
<li>Find the gradient and Hessian of <span class="math inline">\(f\)</span> at the point <span class="math inline">\((1,1)\)</span>.</li>
<li>Find the directional derivative of <span class="math inline">\(f\)</span> at the point <span class="math inline">\((1,1)\)</span> in the direction of the maximal rate of increase.</li>
<li>Find a point that satisfies the first order necessary condition (FONC). Does the point also satisfy the second order necessary condition (SONC) for a minimum?</li>
</ul></li>
<li>Find the stationary points of the function <span class="math display">\[f(x_1,x_2)=\left ({x_1}^2-4\right)^2+{x_2}^2\]</span> Show that <span class="math inline">\(f\)</span> has an absolute minimum at each of the points <span class="math inline">\((x_1,x_2)=(\pm 2,0)\)</span>. Show that the point <span class="math inline">\((0,0)\)</span> is a saddle point.</li>
<li>Show that the point <span class="math inline">\(x^*\)</span> on the line <span class="math inline">\(x_2-2x_1=0\)</span> is a weak global minimiser of <span class="math display">\[f(x)=4{x_1}^2-4x_1x_2+{x_2}^2\]</span></li>
<li>Show that <span class="math display">\[f(x)=3{x_1}^2-{x_2}^2+{x_1}^3\]</span> has a strong local maximiser at
<span class="math inline">\((-2,0)^T\)</span> and a saddle point at <span class="math inline">\((0,0)^T\)</span>, but has no minimisers.</li>
</ol>
<hr />

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="numerical-optimisation-of-univariate-functions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="gradient-methods-for-unconstrained-optimisation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/05-multivariate_unconstrained_optimisation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["OptimisationII_notes.pdf", "OptimisationII_notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
